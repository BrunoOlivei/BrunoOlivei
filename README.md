<!-- Hero curto, claro e orientado a valor -->
# Bruno Oliveira — Data Engineer (Azure • Databricks • Spark)

Crio pipelines **robustos e auditáveis** em Azure (Data Factory + Databricks + Delta Lake), do **ingest** ao **serving**, com foco em **qualidade de dados**, **observabilidade** e **custo**.

- 🔭 Atual: automatizando ingestões em lote/stream para data lakehouse (Unity Catalog), versionando esquemas e PR-based deploy.
- 🎯 Interesses: Lakehouse, Orquestração (ADF/Airflow), PySpark, Data Modeling, RAG para dados tabulares, custo/finops em Spark.
- 📍 Brasil · 🌐 PT/EN · 💼 Aberto a remoto/híbrido.

[LinkedIn](https://www.linkedin.com/in/BrunoOlivei) • [Email](brunoolivei@protonmail.com) • [📄 Currículo (PDF)](./Currículo_Bruno_Oliveira.pdf)

---

## ⚡ Projetos em Destaque
> Cases end-to-end com README completo, diagrama e “como reproduzir”.

- **Lakehouse Seguros (Demo com dados sintéticos)** — *ADF + Databricks + Delta + Unity Catalog*  
  Ingestão Bronze → curadoria Silver → métricas Gold; particionamento, schema evolution, SCD, e checkpoints de qualidade.  
  `azure-data-factory` `databricks` `pyspark` `delta-lake` `unity-catalog`

- **Orquestração de ETL com Airflow (Docker)** — *ELT incremental + testes de dados*  
  DAGs com backfills, SLA, retries, logs centralizados e Great Expectations.

- **Churn Modeling (ML Ops leve)** — *feature store simplificada + tracking*  
  Pipeline de features em Spark, treino com MLflow e serving via FastAPI.

> Dica: fixe (Pin) de 4 a 6 repositórios como estes para contar sua história. :contentReference[oaicite:4]{index=4}

---

## 🧰 Stack e práticas
**Linguagens/Compute:** Python, PySpark, SQL • **Orquestração:** ADF, Airflow  
**Lakehouse/Storage:** Databricks, Delta Lake, Unity Catalog, ADLS  
**Qualidade/Observabilidade:** Great Expectations, checks de schema, logging  
**Dev/CI:** GitHub Actions, testes, padrões de branch e PR com evidências

---

## 📚 Conteúdo e Notas
- Cheatsheets de PySpark/ADF, padrões de nomenclatura, templates de README.
- Estudos: modelagem de dados e particionamento para custos em Spark.
