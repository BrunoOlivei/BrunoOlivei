<!-- Hero curto, claro e orientado a valor -->
# Bruno Oliveira â€” Data Engineer (Azure â€¢ Databricks â€¢ Spark)

Crio pipelines **robustos e auditÃ¡veis** em Azure (Data Factory + Databricks + Delta Lake), do **ingest** ao **serving**, com foco em **qualidade de dados**, **observabilidade** e **custo**.

- ğŸ”­ Atual: automatizando ingestÃµes em lote/stream para data lakehouse (Unity Catalog), versionando esquemas e PR-based deploy.
- ğŸ¯ Interesses: Lakehouse, OrquestraÃ§Ã£o (ADF/Airflow), PySpark, Data Modeling, RAG para dados tabulares, custo/finops em Spark.
- ğŸ“ Brasil Â· ğŸŒ PT/EN Â· ğŸ’¼ Aberto a remoto/hÃ­brido.

[LinkedIn](https://www.linkedin.com/in/BrunoOlivei) â€¢ [Email](brunoolivei@protonmail.com) â€¢ [ğŸ“„ CurrÃ­culo (PDF)](./CurrÃ­culo_Bruno_Oliveira.pdf)

---

## âš¡ Projetos em Destaque
> Cases end-to-end com README completo, diagrama e â€œcomo reproduzirâ€.

- **Lakehouse Seguros (Demo com dados sintÃ©ticos)** â€” *ADF + Databricks + Delta + Unity Catalog*  
  IngestÃ£o Bronze â†’ curadoria Silver â†’ mÃ©tricas Gold; particionamento, schema evolution, SCD, e checkpoints de qualidade.  
  `azure-data-factory` `databricks` `pyspark` `delta-lake` `unity-catalog`

- **OrquestraÃ§Ã£o de ETL com Airflow (Docker)** â€” *ELT incremental + testes de dados*  
  DAGs com backfills, SLA, retries, logs centralizados e Great Expectations.

- **Churn Modeling (ML Ops leve)** â€” *feature store simplificada + tracking*  
  Pipeline de features em Spark, treino com MLflow e serving via FastAPI.

> Dica: fixe (Pin) de 4 a 6 repositÃ³rios como estes para contar sua histÃ³ria. :contentReference[oaicite:4]{index=4}

---

## ğŸ§° Stack e prÃ¡ticas
**Linguagens/Compute:** Python, PySpark, SQL â€¢ **OrquestraÃ§Ã£o:** ADF, Airflow  
**Lakehouse/Storage:** Databricks, Delta Lake, Unity Catalog, ADLS  
**Qualidade/Observabilidade:** Great Expectations, checks de schema, logging  
**Dev/CI:** GitHub Actions, testes, padrÃµes de branch e PR com evidÃªncias

---

## ğŸ“š ConteÃºdo e Notas
- Cheatsheets de PySpark/ADF, padrÃµes de nomenclatura, templates de README.
- Estudos: modelagem de dados e particionamento para custos em Spark.
